{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominant-notification",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-teddy",
   "metadata": {},
   "source": [
    "### Chosen Representation\n",
    "The input csv file was converted into a data frame with abstract and class columns containing the corresponding columns in the csv file.\n",
    "The classes were converted from letters \"A\", \"B\", \"E\", \"V\" to numbers 0, 1, 2, 3 respectively.  \n",
    "These were subsequently converted back into the letters when needing to submit for Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-maine",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "Punctuation was removed from the raw data via `string.punctuation`. This prevents words which should be the same be affected by punctuation. This should make our model more generalisable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-diamond",
   "metadata": {},
   "source": [
    "### Method Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-rating",
   "metadata": {},
   "source": [
    "#### Stopwords\n",
    "Stopwords are words which add to the length of a text but do not contribute to the overall meaning of the text. Some very common example are \"the\" and \"a\". This allows us to focus on more important words which actually differentiate classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-invitation",
   "metadata": {},
   "source": [
    "#### Underflow prevention\n",
    "It is common to encounter numeric instability when using a Naive Bayes classifier. When multiplying many small numbers together, we can encounter underflow. This causes the value of a variable to be set to 0, which prevents any comparisons when choosing a class.\n",
    "\n",
    "The severity of this issue is correlated with input text length and number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-insurance",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Naive Bayes was implemented with smoothing of constant 1.\n",
    "First the data was read in and preprocessed. Next the 1000 most common words were counted using `Counter`. This represents our features for each abstract text.\n",
    "\n",
    "The priors and likelihoods are calculated using the determined features and abstracts. From this it is possible to calculate the posteriors for each word.\n",
    "\n",
    "To find the predicted class for each word, we use the posteriors and priors multiplied together depending on what words are present in the test text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-queen",
   "metadata": {},
   "source": [
    "### Results and Performance\n",
    "\n",
    "10-fold cross validation was implemented to test the models for accurate results. Extended Naive Bayes with stopwords achieved 93% on Kaggle.\n",
    "\n",
    "We can see from the following data that each extension adds more accuracy and reduces the standard deviation of predictions.\n",
    "\n",
    "The most impressive gain in accuracy is from stopwords. This is obvious by looking at the top 10 most common words before removing stop words. Nine out of ten are stop words, with very high frequency counts.\n",
    "`('the', 46507), ('of', 36531), ('and', 24296), ('a', 16588), ('in', 16082), ('to', 12778), ('that', 7743), ('is', 7540), ('genes', 6396), ('with', 6185)`\n",
    "\n",
    "Naive Bayes\n",
    "\n",
    "| Mean  | Median | Standard Deviation |\n",
    "| :--:  |:-----: | :-----: |\n",
    "| 0.527 | 0.539  |  0.0262 |\n",
    "\n",
    "Extended with Stopwords\n",
    "\n",
    "| Mean  | Median | Standard Deviation |\n",
    "| :--:  |:-----: | :-----: |\n",
    "| 0.905 | 0.908  |  0.0144 |\n",
    "\n",
    "Extended with Stopwords + Underflow prevention\n",
    "\n",
    "| Mean  | Median | Standard Deviation |\n",
    "| :--:  |:-----: | :-----: |\n",
    "| 0.910 | 0.908  |  0.0142 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-marshall",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-click",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "about-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np \n",
    "def readCsv(filename):\n",
    "    with open(f'{filename}.csv', newline='') as csvfile:\n",
    "        data = list(csv.reader(csvfile))\n",
    "    data = np.array(data[1:])\n",
    "    return data\n",
    "data = readCsv('trg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-choice",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-provincial",
   "metadata": {},
   "source": [
    "We want to convert the letter values for the classes into ints so they are easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "binary-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, clazz in enumerate(data[:,1]):\n",
    "    if clazz == \"A\":\n",
    "        data[i][1] = 0\n",
    "    elif clazz == \"B\":\n",
    "        data[i][1] = 1\n",
    "    elif clazz == \"E\":\n",
    "        data[i][1] = 2\n",
    "    elif clazz == \"V\":\n",
    "        data[i][1] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-quilt",
   "metadata": {},
   "source": [
    "Next, we want to remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exceptional-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "counts = Counter()\n",
    "\n",
    "for i, abstract in enumerate(data[:,2]):\n",
    "    # Remove punctuation\n",
    "    abstract = abstract.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = abstract.split()\n",
    "    counts.update(words)\n",
    "    data[i,2] = \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "active-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = data\n",
    "data = {}\n",
    "data['class'] = [int(clazz) for clazz in rawData[:, 1]]\n",
    "data['abstract'] = rawData[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "static-application",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "print(len(data['abstract']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-liverpool",
   "metadata": {},
   "source": [
    "### Construct Attributes\n",
    "Top 1000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "protective-browser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31424\n",
      "[('the', 46507), ('of', 36531), ('and', 24296), ('a', 16588), ('in', 16082), ('to', 12778), ('that', 7743), ('is', 7540), ('genes', 6396), ('with', 6185), ('for', 5830), ('sequence', 5184), ('gene', 5097), ('from', 5081), ('are', 4901), ('was', 4488), ('by', 4317), ('genome', 3728), ('protein', 3503), ('were', 3428), ('as', 3291), ('this', 3136), ('which', 3048), ('an', 3046), ('we', 3030), ('have', 2757), ('amino', 2635), ('two', 2590), ('these', 2549), ('proteins', 2520), ('sequences', 2430), ('acid', 2128), ('human', 2049), ('has', 2048), ('be', 2036), ('dna', 1993), ('been', 1914), ('other', 1843), ('on', 1823), ('at', 1809), ('analysis', 1792), ('cdna', 1774), ('identified', 1645), ('region', 1545), ('or', 1410), ('found', 1384), ('not', 1353), ('chromosome', 1349), ('expression', 1289), ('also', 1250)]\n"
     ]
    }
   ],
   "source": [
    "print(len(counts))\n",
    "print(counts.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "obvious-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAttributes(abstracts):\n",
    "    word_counts = Counter()\n",
    "    for abstract in abstracts:\n",
    "        word_list = abstract.split()\n",
    "        word_counts.update(word_list)\n",
    "    attributes = [word for word,count in counts.most_common(1000)]\n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expressed-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(attributes, abstracts):\n",
    "    feature_counts = []\n",
    "    for abstract in abstracts:\n",
    "        new_row = []\n",
    "        for word in attributes:\n",
    "            new_row.append(abstract.count(word))\n",
    "        feature_counts.append(new_row)\n",
    "    return feature_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "immune-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPriors(classes):\n",
    "    priors = Counter(classes)\n",
    "    for prior in priors:\n",
    "        priors[prior] /= len(classes)\n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "powerful-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLikelihoods(priors, attributes, classes, abstracts):\n",
    "    # Generate word counts for each class\n",
    "    counts_for_class = [Counter() for i in range(4)]\n",
    "    for clazz, abstract in zip(classes, abstracts):\n",
    "        cnts = Counter([word for word in abstract.split() if word in attributes])\n",
    "        counts_for_class[int(clazz)].update(cnts)\n",
    "    # Convert these word counts into likelihoods per class\n",
    "    likelihoods = [dict() for i in range(4)]\n",
    "    for clazz in priors:\n",
    "        clazz = int(clazz)\n",
    "        for word in attributes:\n",
    "            likelihood = (counts_for_class[clazz][word] + 1) / (sum(counts_for_class[clazz].values())+len(attributes))\n",
    "            likelihoods[clazz][word] = likelihood\n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fatty-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbabilityData(data):\n",
    "    classes = data['class']\n",
    "    abstracts = data['abstract']\n",
    "    attributes = getAttributes(abstracts)\n",
    "    \n",
    "    feature_counts = extractFeatures(attributes, abstracts)\n",
    "    priors = getPriors(classes)\n",
    "    likelihoods = getLikelihoods(priors, attributes, classes, abstracts)\n",
    "    \n",
    "    return attributes, priors, likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-behavior",
   "metadata": {},
   "source": [
    "The `attributes` variable represents the top 1000 most common words in the abstracts data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-jewel",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "experimental-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_posterior_naive(clazz, priors, likelihoods, words):\n",
    "        posterior = priors[clazz]\n",
    "        for word in words:\n",
    "            posterior *= likelihoods[clazz].get(word)\n",
    "        return posterior\n",
    "    \n",
    "import math\n",
    "def calculate_posterior_log(clazz, priors, likelihoods, words):\n",
    "        posterior = math.log(float(priors[clazz]))\n",
    "        for word in words:\n",
    "            posterior += math.log(likelihoods[clazz].get(word))\n",
    "        return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "willing-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePrediction(test_abstract, likelihoods, priors, attributes, log=False):\n",
    "    test_words = [word for word in test_abstract.split() if word in attributes]\n",
    "    \n",
    "    posteriors = []\n",
    "    for clazz in range(4):\n",
    "        if log:\n",
    "            posterior = calculate_posterior_log(clazz, priors, likelihoods, test_words)\n",
    "        else: \n",
    "            posterior = calculate_posterior_naive(clazz, priors, likelihoods, test_words)\n",
    "        posteriors.append(posterior)\n",
    "    return np.argmax(posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "infinite-detective",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def makePredictions(abstracts, likelihoods, priors, attributes, log=False):\n",
    "    predictions = []\n",
    "    for abstract in abstracts:\n",
    "        prediction = makePrediction(abstract, likelihoods, priors, attributes, log)\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "previous-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkAccuracy(predictions, actualClasses):\n",
    "    totalCorrect = 0\n",
    "    for prediction, actualClass in zip(predictions, actualClasses):\n",
    "        if prediction == int(actualClass):\n",
    "            totalCorrect += 1\n",
    "    return totalCorrect / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "moving-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainTestSplit(start, end, data):\n",
    "    test = {}\n",
    "    test['class'] = data['class'][start: end]\n",
    "    test['abstract'] = data['abstract'][start:end]\n",
    "    \n",
    "    train = {}\n",
    "    train['abstract'] = np.concatenate((data['abstract'][:start], data['abstract'][end:]))\n",
    "    train['class'] = np.concatenate((data['class'][:start], data['class'][end:]))\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def crossValidate(data, folds=10, log=False):\n",
    "    data['class'] = [int(clazz) for clazz in data['class']]\n",
    "    percentageTrain = 1 / folds \n",
    "    trainLen = int(len(data['class']) * percentageTrain)\n",
    "    \n",
    "    accuracies = []\n",
    "    for fold in range(folds):\n",
    "        start = fold * trainLen\n",
    "        end = start + trainLen\n",
    "        train, test = getTrainTestSplit(start, end, data)\n",
    "        attributes, priors, likelihoods = getProbabilityData(train)\n",
    "        predictions = makePredictions(test['abstract'], likelihoods, priors, attributes, log)\n",
    "        accuracy = checkAccuracy(predictions, test['class'])\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        accuracies.append(accuracy)\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-support",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "universal-elder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4675\n",
      "Accuracy: 0.5425\n",
      "Accuracy: 0.5025\n",
      "Accuracy: 0.5475\n",
      "Accuracy: 0.5225\n",
      "Accuracy: 0.55\n",
      "Accuracy: 0.5525\n",
      "Accuracy: 0.545\n",
      "Accuracy: 0.505\n",
      "Accuracy: 0.535\n"
     ]
    }
   ],
   "source": [
    "accuracies = crossValidate(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "laden-quarterly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5269999999999999\n",
      "0.02621545345783666\n",
      "0.5387500000000001\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(accuracies))\n",
    "print(np.std(accuracies))\n",
    "print(np.median(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-trainer",
   "metadata": {},
   "source": [
    "#### With Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-mistress",
   "metadata": {},
   "source": [
    "Remove all stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adult-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"able\",\"about\",\"above\",\"abroad\",\"according\",\"accordingly\",\"across\",\"actually\",\"adj\",\"after\",\"afterwards\",\"again\",\"against\",\"ago\",\"ahead\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"alongside\",\"already\",\"also\",\"although\",\"always\",\"am\",\"amid\",\"amidst\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"a's\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"back\",\"backward\",\"backwards\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"begin\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"came\",\"can\",\"cannot\",\"cant\",\"can't\",\"caption\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"c'mon\",\"co\",\"co.\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"c's\",\"currently\",\"dare\",\"daren't\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"directly\",\"do\",\"does\",\"doesn't\",\"doing\",\"done\",\"don't\",\"down\",\"downwards\",\"during\",\"each\",\"edu\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"evermore\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"fairly\",\"far\",\"farther\",\"few\",\"fewer\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"forever\",\"former\",\"formerly\",\"forth\",\"forward\",\"found\",\"four\",\"from\",\"further\",\"furthermore\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"had\",\"hadn't\",\"half\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he'd\",\"he'll\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"hereafter\",\"hereby\",\"herein\",\"here's\",\"hereupon\",\"hers\",\"herself\",\"he's\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"hundred\",\"i'd\",\"ie\",\"if\",\"ignored\",\"i'll\",\"i'm\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"inc.\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"inside\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"its\",\"it's\",\"itself\",\"i've\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"likewise\",\"little\",\"look\",\"looking\",\"looks\",\"low\",\"lower\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mayn't\",\"me\",\"mean\",\"meantime\",\"meanwhile\",\"merely\",\"might\",\"mightn't\",\"mine\",\"minus\",\"miss\",\"more\",\"moreover\",\"most\",\"mostly\",\"mr\",\"mrs\",\"much\",\"must\",\"mustn't\",\"my\",\"myself\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needn't\",\"needs\",\"neither\",\"never\",\"neverf\",\"neverless\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"no\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"no-one\",\"nor\",\"normally\",\"not\",\"nothing\",\"notwithstanding\",\"novel\",\"now\",\"nowhere\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"one's\",\"only\",\"onto\",\"opposite\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"oughtn't\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provided\",\"provides\",\"que\",\"quite\",\"qv\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"recent\",\"recently\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"round\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"shan't\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"someday\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that'll\",\"thats\",\"that's\",\"that've\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"thereafter\",\"thereby\",\"there'd\",\"therefore\",\"therein\",\"there'll\",\"there're\",\"theres\",\"there's\",\"thereupon\",\"there've\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"thing\",\"things\",\"think\",\"third\",\"thirty\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"till\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"t's\",\"twice\",\"two\",\"un\",\"under\",\"underneath\",\"undoing\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"upwards\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"versus\",\"very\",\"via\",\"viz\",\"vs\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"welcome\",\"well\",\"we'll\",\"went\",\"were\",\"we're\",\"weren't\",\"we've\",\"what\",\"whatever\",\"what'll\",\"what's\",\"what've\",\"when\",\"whence\",\"whenever\",\"where\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"where's\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"whichever\",\"while\",\"whilst\",\"whither\",\"who\",\"who'd\",\"whoever\",\"whole\",\"who'll\",\"whom\",\"whomever\",\"who's\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"wonder\",\"won't\",\"would\",\"wouldn't\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"your\",\"you're\",\"yours\",\"yourself\",\"yourselves\",\"you've\",\"zero\",\"a\",\"how's\",\"i\",\"when's\",\"why's\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"j\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"uucp\",\"w\",\"x\",\"y\",\"z\",\"I\",\"www\",\"amount\",\"bill\",\"bottom\",\"call\",\"computer\",\"con\",\"couldnt\",\"cry\",\"de\",\"describe\",\"detail\",\"due\",\"eleven\",\"empty\",\"fifteen\",\"fifty\",\"fill\",\"find\",\"fire\",\"forty\",\"front\",\"full\",\"give\",\"hasnt\",\"herse\",\"himse\",\"interest\",\"itse”\",\"mill\",\"move\",\"myse”\",\"part\",\"put\",\"show\",\"side\",\"sincere\",\"sixty\",\"system\",\"ten\",\"thick\",\"thin\",\"top\",\"twelve\",\"twenty\",\"abst\",\"accordance\",\"act\",\"added\",\"adopted\",\"affected\",\"affecting\",\"affects\",\"ah\",\"announce\",\"anymore\",\"apparently\",\"approximately\",\"aren\",\"arent\",\"arise\",\"auth\",\"beginning\",\"beginnings\",\"begins\",\"biol\",\"briefly\",\"ca\",\"date\",\"ed\",\"effect\",\"et-al\",\"ff\",\"fix\",\"gave\",\"giving\",\"heres\",\"hes\",\"hid\",\"home\",\"id\",\"im\",\"immediately\",\"importance\",\"important\",\"index\",\"information\",\"invention\",\"itd\",\"keys\",\"kg\",\"km\",\"largely\",\"lets\",\"line\",\"'ll\",\"means\",\"mg\",\"million\",\"ml\",\"mug\",\"na\",\"nay\",\"necessarily\",\"nos\",\"noted\",\"obtain\",\"obtained\",\"omitted\",\"ord\",\"owing\",\"page\",\"pages\",\"poorly\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"promptly\",\"proud\",\"quickly\",\"ran\",\"readily\",\"ref\",\"refs\",\"related\",\"research\",\"resulted\",\"resulting\",\"results\",\"run\",\"sec\",\"section\",\"shed\",\"shes\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"slightly\",\"somethan\",\"specifically\",\"state\",\"states\",\"stop\",\"strongly\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"thered\",\"thereof\",\"therere\",\"thereto\",\"theyd\",\"theyre\",\"thou\",\"thoughh\",\"thousand\",\"throug\",\"til\",\"tip\",\"ts\",\"ups\",\"usefully\",\"usefulness\",\"'ve\",\"vol\",\"vols\",\"wed\",\"whats\",\"wheres\",\"whim\",\"whod\",\"whos\",\"widely\",\"words\",\"world\",\"youd\",\"youre\"]\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "for i, abstract in enumerate(data['abstract']):\n",
    "    words = abstract.split()\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    data['abstract'][i] = \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "italian-lingerie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9075\n",
      "Accuracy: 0.8775\n",
      "Accuracy: 0.8875\n",
      "Accuracy: 0.89\n",
      "Accuracy: 0.9075\n",
      "Accuracy: 0.91\n",
      "Accuracy: 0.905\n",
      "Accuracy: 0.915\n",
      "Accuracy: 0.9175\n",
      "Accuracy: 0.9275\n"
     ]
    }
   ],
   "source": [
    "accuracies = crossValidate(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "unauthorized-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9045\n",
      "0.014439529078193665\n",
      "0.9075\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(accuracies))\n",
    "print(np.std(accuracies))\n",
    "print(np.median(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-offset",
   "metadata": {},
   "source": [
    "#### Underflow prevention + Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "minor-district",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9075\n",
      "Accuracy: 0.8775\n",
      "Accuracy: 0.9025\n",
      "Accuracy: 0.905\n",
      "Accuracy: 0.9075\n",
      "Accuracy: 0.93\n",
      "Accuracy: 0.9075\n",
      "Accuracy: 0.92\n",
      "Accuracy: 0.9175\n",
      "Accuracy: 0.9275\n"
     ]
    }
   ],
   "source": [
    "accuracies = crossValidate(data, 10, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "clear-humor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9102500000000001\n",
      "0.014206072645175396\n",
      "0.9075\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(accuracies))\n",
    "print(np.std(accuracies))\n",
    "print(np.median(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-amplifier",
   "metadata": {},
   "source": [
    "### Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "forward-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changePredictionToLetterClass(predictions):\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        if prediction == 0:\n",
    "            predictions[i] = \"A\"\n",
    "        elif prediction == 1:\n",
    "            predictions[i] = \"B\"\n",
    "        elif prediction == 2:\n",
    "            predictions[i] = \"E\"\n",
    "        elif prediction == 3:\n",
    "            predictions[i] = \"V\"\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eligible-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggleSubmission(data):\n",
    "    test_data = readCsv('tst')\n",
    "    attributes, priors, likelihoods = getProbabilityData(data)\n",
    "    predictions = makePredictions(test_data[:,1], likelihoods, priors, attributes, log=True)\n",
    "    predictions = changePredictionToLetterClass(predictions)\n",
    "\n",
    "    output = list(zip(range(1,1001), predictions))\n",
    "\n",
    "    with open('submission.csv', 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow((\"id\", \"class\"))\n",
    "        for row in output:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "assumed-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggleSubmission(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-chrome",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
